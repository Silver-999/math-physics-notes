\documentclass[12pt]{article}
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{booktabs}
\usepackage{array}
\usepackage{graphicx}
\usepackage{multirow}

\begin{document}

\begin{center}
    \Large\textbf{Probability Theory and Mathematical Statistics}
\end{center}

\section*{I. Probability Formulas}
\begin{itemize}
    \item \textbf{Addition Formula}: $P(A + B) = P(A \cup B) = P(A) + P(B) - P(AB)$
    \item \textbf{Subtraction Formula}: $P(A - B) = P(A\overline{B}) = P(A) - P(AB)$
    \item \textbf{Conditional Probability}: $P(A|B) = \dfrac{P(AB)}{P(B)}$, $P(B) > 0$
    \item \textbf{Compatible Events}: $P(AB) > 0$
    \item \textbf{Mutually Exclusive Events}: $P(AB) = 0$
    \item \textbf{Independent Events}: $P(AB) = P(A)P(B)$
    \item \textbf{Distribution Function}:
    \begin{itemize}
        \item $F(a) = P\{X \leq a\}$
        \item $P\{X < a\} = \displaystyle\lim_{x \to a^-} F(x)$
        \item $f_X(x) = \dfrac{\mathrm{d}F_X(x)}{\mathrm{d}x}$
    \end{itemize}
    \item \textbf{Convolution Function} ($Z=X+Y$): $f_Z(z) = \int_{-\infty}^\infty f_X(x) f_Y(z-x) \,\mathrm{d}x$
    \item \textbf{Convolution Function} ($Z = X - Y$): $f_Z(z) = \int_{-\infty}^{\infty} f_X(x) \, f_Y(x - z) \, \mathrm{d}x$
\end{itemize}

\section*{II. Numerical Characteristics}
\begin{itemize}
    \item \textbf{Mathematical Expectation}:
    \begin{itemize}
        \item Continuous Type: If the probability density of $X$ is $f(x)$, then $\displaystyle E(X) = \int_{-\infty}^{+\infty} x f(x) \, \mathrm{d}x$
        \item Function of Random Variable: $\displaystyle E[g(X)] = \int_{-\infty}^{+\infty} g(x) f(x) \, \mathrm{d}x$
    \end{itemize}

    \item \textbf{Marginal Expectation in Two-Dimensional Case}: If the joint probability density of $(X, Y)$ is $f(x, y)$, its marginal probability densities are:
        \begin{align*}
            f_X(x) &= \int_{-\infty}^{+\infty} f(x, y) \, \mathrm{d}y \\
            f_Y(y) &= \int_{-\infty}^{+\infty} f(x, y) \, \mathrm{d}x
        \end{align*}

    \item \textbf{Variance}:
    \begin{itemize}
        \item Definition: $D(X) = E[(X - E(X))^2]$
        \item Computational Formula: $D(X) = E(X^2) - [E(X)]^2$
        \item Property: $D(CX) = C^2 D(X)$
    \end{itemize}

    \item \textbf{Correlation Coefficient}: $\displaystyle \rho_{XY} = \frac{\mathrm{Cov}(X,Y)}{\sqrt{D(X)} \cdot \sqrt{D(Y)}}$

    \item \textbf{Equivalent Propositions of Uncorrelated}:
    \[
    \mathrm{Cov}(X,Y) = 0 \iff \rho_{XY} = 0 \iff E(XY) = E(X)E(Y) \iff D(X+Y) = D(X) + D(Y)
    \]

    \item \textbf{Covariance}:
    \begin{itemize}
        \item Definition: $\displaystyle \mathrm{Cov}(X,Y) = E[(X - E(X))(Y - E(Y))]$
        \item Computational Formula: $\displaystyle \mathrm{Cov}(X,Y) = E(XY) - E(X)E(Y)$
    \end{itemize}
    
    \item \textbf{Properties of Covariance}
    \begin{enumerate}
        \item $\mathrm{Cov}(X,Y) = \mathrm{Cov}(Y,X)$
        \item $\mathrm{Cov}(X,C) = 0$
        \item $\mathrm{Cov}(aX,bY) = ab\,\mathrm{Cov}(X,Y)$
        \item $\mathrm{Cov}(X,X) = D(X)$
        \item $\mathrm{Cov}(X+Y,Z) = \mathrm{Cov}(X,Z) + \mathrm{Cov}(Y,Z)$
        \item $D(X \pm Y) = D(X) + D(Y) \pm 2\,\mathrm{Cov}(X,Y)$
        \item $X, Y$ are independent $\Rightarrow \mathrm{Cov}(X,Y) = 0$
    \end{enumerate}
\end{itemize}

\section*{III. Probability Distributions}
\begin{itemize}
    \item \textbf{Poisson Distribution}: $X \sim P(\lambda)$
    \begin{itemize}
        \item Conclusion:
        If $X \sim P(\lambda_1)$, $Y \sim P(\lambda_2)$, and $X$ and $Y$ are independent, then $X + Y \sim P(\lambda_1 + \lambda_2)$
    \end{itemize}
    
    \item \textbf{Normal Distribution}: $X \sim N(\mu, \sigma^2)$
    \begin{itemize}
        \item Standardization and Probability:
        \[
        P\{a < X \leq b\} = P\left\{\frac{a - \mu}{\sigma} < \frac{X - \mu}{\sigma} \leq \frac{b - \mu}{\sigma}\right\} = \Phi\left(\frac{b - \mu}{\sigma}\right) - \Phi\left(\frac{a - \mu}{\sigma}\right)
        \]
        \item Conclusion: If $Z \sim N(0,1)$, then
        \[
        \forall a > 0, \quad P\{|Z| \leq a\} = 2\Phi(a) - 1
        \]
    \end{itemize}

    \item \textbf{Exponential Distribution}: $X \sim E(\lambda)$ ($\lambda > 0$)
    \begin{itemize}
        \item Conclusion:
        \begin{itemize}
            \item $P\{X > a\} = e^{-\lambda a} \quad (a > 0)$
            \item $P\{X > s + t \mid X > s\} = P\{X > t\}$, where $s, t > 0$
        \end{itemize}
    \end{itemize}

\newpage

    \item \textbf{Common Distributions Table}:
    \begin{table}[h]
        \centering
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Type} & \textbf{Notation} & \textbf{Distribution Law/Density} & \textbf{Expectation} & \textbf{Variance} \\
        \hline
        0-1 & $X \sim b(1,p)$ & $P\{X = k\} = p^k (1-p)^{1-k}$ \newline $k=0,1$ & $p$ & $p(1-p)$ \\
        \hline
        Binomial & $X \sim B(n,p)$ & $P\{X = k\} = C_n^k p^k (1-p)^{n-k}$ \newline $k=0,1,\dots,n$ & $np$ & $np(1-p)$ \\
        \hline
        Poisson & $X \sim \pi (\lambda)$ & $P\{X = k\} = \frac{\lambda^k}{k!} e^{-\lambda}$ \newline $k=0,1,2,\dots$ & $\lambda$ & $\lambda$ \\
        \hline
        Uniform & $X \sim U(a,b)$ & $f(x) = 
        \begin{cases} 
        \frac{1}{b-a}, & a < x < b \\ 
        0, & \text{otherwise}
        \end{cases}$ & $\frac{a+b}{2}$ & $\frac{(b-a)^2}{12}$ \\
        \hline
        Normal & $X \sim N(\mu,\sigma^2)$ & $f(x) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ \newline $-\infty < x < +\infty$ & $\mu$ & $\sigma^2$ \\
        \hline
        Exponential & $X \sim E(\lambda)$ & $f(x) = 
        \begin{cases} 
        \lambda e^{-\lambda x}, & x > 0 \\ 
        0, & x \leq 0
        \end{cases}$ & $\frac{1}{\lambda}$ & $\frac{1}{\lambda^2}$ \\
        \hline
        \end{tabular}
    \end{table}
\end{itemize}

\section*{IV. Mathematical Statistics}
\begin{itemize}
    \item \textbf{Common Statistics}:
    \begin{itemize}
        \item Sample Mean: $\displaystyle \overline{X} = \frac{1}{n} \sum_{i=1}^{n} X_i$
        \item Sample Variance: $\displaystyle S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \overline{X})^2$
    \end{itemize}
    
    \item \textbf{Three Major Sampling Distributions}:
    \begin{itemize}
        \item $\chi^2$ \textbf{Distribution}:
        \begin{itemize}
            \item Definition: $X_1, \dots, X_n \stackrel{\text{i.i.d.}}{\sim} N(0,1)$, then $\displaystyle \sum_{i=1}^n X_i^2 \sim \chi^2(n)$
            \item Property: $E(\chi^2) = n,\; D(\chi^2) = 2n$
        \end{itemize}
        
        \item $t$ \textbf{Distribution}:
        \begin{itemize}
            \item Definition: $X \sim N(0,1), Y \sim \chi^2(n)$ independent, then $\displaystyle T = \frac{X}{\sqrt{Y/n}} \sim t(n)$
            \item Property: Probability density $f(t)$ is an even function; if $T \sim t(n)$, then $T^2 \sim F(1,n)$
        \end{itemize}
        
        \item $F$ \textbf{Distribution}:
        \begin{itemize}
            \item Definition: $X \sim \chi^2(n_1), Y \sim \chi^2(n_2)$ independent, then $\displaystyle F = \frac{X/n_1}{Y/n_2} \sim F(n_1, n_2)$
            \item Property: If $F \sim F(n_1, n_2)$, then $\displaystyle \frac{1}{F} \sim F(n_2, n_1)$
        \end{itemize}
    \end{itemize}

    \item \textbf{Properties of Statistics}:
    Let $X_1, X_2, \dots, X_n$ be a sample from population $X$, with $E(X) = \mu$, $D(X) = \sigma^2$, then:
        \begin{itemize}
        \item $E(X_i) = \mu,\; D(X_i) = \sigma^2,\; E(\overline{X}) = \mu,\; D(\overline{X}) = \dfrac{\sigma^2}{n}$
        \end{itemize}

\newpage
    
  \item \textbf{Testing Methods for Mean and Variance of Normal Population}:
    \begin{table}[h]
        \centering
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{Null Hypothesis $H_0$} & \textbf{Test Statistic} & \textbf{Rejection Region} \\
            \hline
            $\mu \leq \mu_0$ & \multirow{3}{*}{$\displaystyle Z = \frac{\overline{X} - \mu_0}{\sigma / \sqrt{n}}$} & $z \geq z_\alpha$ \\
            \cline{1-1} \cline{3-3}
            $\mu = \mu_0$ & & $|z| \geq z_{\alpha/2}$ \\
            \cline{1-1} \cline{3-3}
            $\mu \geq \mu_0$ & & $z \leq -z_\alpha$ \\
            \hline
            $\mu \leq \mu_0$ & \multirow{3}{*}{$\displaystyle t = \frac{\overline{X} - \mu_0}{S / \sqrt{n}}$} & $t \geq t_\alpha(n-1)$ \\
            \cline{1-1} \cline{3-3}
            $\mu = \mu_0$ & & $|t| \geq t_{\alpha/2}(n-1)$ \\
            \cline{1-1} \cline{3-3}
            $\mu \geq \mu_0$ & & $t \leq -t_\alpha(n-1)$ \\
            \hline
            $\sigma^2 \leq \sigma_0^2$ & \multirow{3}{*}{$\displaystyle \chi^2 = \frac{(n-1)S^2}{\sigma_0^2}$} & $\chi^2 \geq \chi_\alpha^2(n-1)$ \\
            \cline{1-1} \cline{3-3}
            $\sigma^2 = \sigma_0^2$ & & $\chi^2 \leq \chi_{1-\alpha/2}^2(n-1)$ or $\chi^2 \geq \chi_{\alpha/2}^2(n-1)$ \\
            \cline{1-1} \cline{3-3}
            $\sigma^2 \geq \sigma_0^2$ & & $\chi^2 \leq \chi_{1-\alpha}^2(n-1)$ \\
            \hline
        \end{tabular}
    \end{table}

\end{itemize}

\section*{V. Parameter Estimation and Hypothesis Testing}
\begin{itemize}
    \item \textbf{Unbiasedness}:
    If $E(\hat{\theta}) = \theta$, then $\hat{\theta}$ is called an unbiased estimator of $\theta$.
    
    \item \textbf{Efficiency}:
    If $\hat{\theta}_1$ and $\hat{\theta}_2$ are both unbiased estimators of $\theta$, and $D(\hat{\theta}_1) < D(\hat{\theta}_2)$, then $\hat{\theta}_1$ is more efficient than $\hat{\theta}_2$.
    
    \item \textbf{Method of Moments}: $E(X) = \overline{X}$
    \begin{itemize}
        \item $\int_{-\infty}^{+\infty} x f(x) \, \mathrm{d}x = \frac{1}{n}\sum_{i=1}^n X_i$
    \end{itemize}

    \item \textbf{Steps of Maximum Likelihood Estimation}:
    \begin{itemize}
        \item $L(\theta) = \displaystyle\prod_{i=1}^n p(x_i; \theta) \Rightarrow \ln L(\theta) = \displaystyle\sum_{i=1}^n \ln p(x_i; \theta) \Rightarrow \displaystyle\frac{\mathrm{d}[\ln L(\theta)]}{\mathrm{d}\theta} = 0$
    \end{itemize}
    
    \item \textbf{Confidence Interval for $\mu$ of Single Normal Population} (Confidence Level $1-\alpha$):
    \begin{enumerate}
        \item $\sigma^2$ known: $\displaystyle \left(\overline{X} - Z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}},\; \overline{X} + Z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}\right)$
        \item $\sigma^2$ unknown: $\displaystyle \left(\overline{X} - t_{\alpha/2}(n-1) \cdot \frac{S}{\sqrt{n}},\; \overline{X} + t_{\alpha/2}(n-1) \cdot \frac{S}{\sqrt{n}}\right)$
    \end{enumerate}

    \item \textbf{Two Types of Errors in Hypothesis Testing}:
    \begin{itemize}
        \item \textbf{Type I Error (Rejecting Truth)}: When $H_0$ is true, $H_0$ is rejected.
        \item \textbf{Type II Error (Accepting False)}: When $H_0$ is false, $H_0$ is accepted.
    \end{itemize}

\end{itemize}

\end{document}